
1@¥$Host. py
From socket impart AF_INET, gethostbynane, gethostbyaddr
from ..config import DOMATN_FQOH
from pydantic import BaseModel, constr
from shaulmiko import ShaulMiko
rom fastapi inport status
from fastapi.exceptions import HTTPException
class Host(BaseModel) :
hostname: constr{max_length=256} = None
ip: constr(max_length=256) = None
os: constr(max_length=256) = None
network: constr(max_length=256) = None
region: conste(max_length=256) = None
connection: ShaulMiko = None

en
region: constr(max_length=256) = None
connection: ShaulMiko = None
def init__(self, **data):
super(}.__init__(**data)
if self-hostname:
if °." not in self.hostname:
self.hostname += £”,{DOMAIN_FQDN}”
class Config:
arbitrary_types_allowed = True
def dns_to_ip(self):
Sets ip property of Host from dns and returns the value
if "." not in self.hostname:
self.hostname += £7.{DOMAIN_FQON}”
self.ip = gethostbynane(f”{self.hostname}")
return self.ip
def ip_to_dns(self):
Sets ip property of Host from dns and returns the value
res = gethostbyaddr(self.ip)
self.hostname = res.name
return self hostname
I
def _execute(self, command):
if self.connection:
return self.connection.execute(conmand, clean_prompt-True) .strip()
raise HTTPException(status.HTTP_408 REQUEST TIMEOUT, “No connection connect to Host”)
def _connect(self, username, password) -> ShaulMiko:
try:
self.connection = ShaulMiko(self.hostname, 22, username, password)
return self.connection
except Exception as e:
sel¥.connection = None
raise HITPException(status.HTTP_417_EXPECTATIOM FAILED, f"Couldn't connect to Host {self.hostnane}: {str(e)}")
def _execute_no_cutput(self, command):
if self.connection:
return self.connection.execute(conmand, timeout=8)
raise WITPException(status.HTTP_498_REQUEST_TIMEQUT, “No connection to Host")

ee
if self.connection:
return self.connection.execute(command, timeout-2)
raise HITPEXception(status.HTTP_408_REQUEST_TIMEOUT, “No connection to Host”)
def _connect_and_execute(self, username, password, command):
self._connect(usernane, password)
return self._execute(command)
def _connect_and_execute_no_output(self, username, password, command) :
if not self.connection:
self._connect(username, password)
self.connection.execute(comand, timeout=8)
return #*Sent Command {command}"
def get_cpu_usage(self):
return self._execute(“top -bni | grep \"Cpu(s)\" | sed \"5/-*, *\\([8-9. J*\)R* id .*/\\I/\" | ak {print 180 - $5\"R\"}'*)
def get_nemory_usage(self}:
return self._execute("free | awk */Men/{printf(\"%.2f%\", $3/$2*180)}"")
def get_ol1_disks_usage(self):
out = {}
disks = self._execute(“df -h | awk "{print S6\":\"S5\"V7}'")
try:
for disk in disks.split("\n"):
if "Usex” in disk:
continue
out[disk.split(":"){@]] = disk. split(":"}[1]
return out I
except:
return “N/A”
def get_root_usage(self):
return self._execute(“df -h | awk “$NF=<\"/\"{printf \"K.2*%\", $5}'")
def get_io(self):
lines = self._execute(“iostat -x | awk “WR>G{print $1,$6,$7,$8}'")
data = {}
for line in lines.split(”\n"):
if line:
fields = line.split()
device = Fields[e]
data[device] = {
“r_per_s": float(fields[1]),
“wper_s": float(fields(2]),
“r_merged_per_s":float(Fields[3])
.

re ee
“p_per_s": float(fields[1]),
“wper_s": float(fields[2]),
“r_merged_per_s”:float(fields[3))
}
return data
lensKafka.py
import json
from fastapi.exceptions import HTTPException
from fastapi import status
from .Host import Host
from -Zookeeper import. Zookeeper
from typing inport List
from ..config import VICTORIA _QUERY_METRICS URL, REQUESTS_CLIENT, KAFKA_AUTH, REASSIGN COMMAND
ron pydantic import BaseModel, constr
class Broker(Host):
id: constr(max_length=256) = None
cluster: constr(max_length=256) = None
listen_port: constr(max_length=5) = ~9092"
up: bool = True
connected_to cluster: bool = True
def _init_(self, **data):
super()-__init__(**data)
self.get_broker_state()
def connect_to_broker(self):
self._connect(KAFKA_AUTH["username”], KAFKA_AUTH["password™])
return True
I
def get_broker_state(self):
stat = REQUESTS CLIENT. get (VICTORIA QUERY_METRICS URL + ¥‘kafka_broker_info{{address="{self. hostname}: {self.listen_port}"}}').json()("data"]["result”]
if not stat:
raise HTTPException(status.HTTP_4@0_BAD_REQUEST, “Braker Doesn‘t exist")
try:
self.connect_to_broker()
except:
self.up = False
try:
self.connected_to cluster = bool(stat[@]["value”}{1})
except:
self.connected_to_cluster = False
if self.connection:
try:
if len(self._execute("ps -ef { grep java.*kafka | grep -v grep”)) > 10:
self.up = True
else:
self.up = False

een nee me
self.up = True
else:
self.up = False
except:
self.up = False
def stop(self):
self._connect_and_execute_no_output(“pkill -9 .*java.*kafka.*")
return True
def start(self):
if self.up:
return HTTPException(status HTTP_2@8 ALREADY REPORTED, “Broker is already up")
else:
if self.connect_to_broker():
return self._execute("/home/kafka/scripts/daemon-kafka-start")
class KafkaCluster(BaseModel):
rane: constr(max_length-256) = None
brokers: List{Broker] = []
zookeepers: List[Zookeeper] = [1
region: constr(max_length=256) = None
network: constr(max_length-256) = None
def _init_(sel¥, **data):
super(}.__init__(**data)
self.get_brokers()
self.get_zookeepers()
““" an example if ever going full async I
@classmethod
async def create(cls, **data):
self = cls(**data)
await asyncio.gather(self.get_brokers(), self.get_zookeepers())
return self""”
def get_brokers(self):
info = REQUESTS_CLIENT.get (VICTORTA_QUERY_METRICS_URL + f*kafka_broker_info{{cluster="{self.name}"}}*).json()
for broker in info[“data"}["result”]:
self. brokers .append(Broker(hostnane=broker[ “metric” }["address"].split(":")[®], cegion-broker["metric™]["site"],
Listen_port=broker[ “metric” ]["address" ]-split(":")[1], state-broker[“value”]{1]})
return True
def get_zookeepers(self):
nfo = REQUESTS_CLIENT.get(VICTORTA_QUERY_METRICS_URL + #‘up{{job="zookeeper", cluster="{self.nane}"}}').Json()
zookeeper_list = “"
for zook in infof"data"] (“result”):
self. zookeepers. append (Zookeeper(hostnane=z00k{ “aetric")["instance”] split(":"}[@J, region-zook{”metric"]["site"]))
nang ewe TE tet a hasennmnt fer & aanbanmeanef A¥ Viekan mantel ®

ene oe a
zookeeper_list = ""
for z0ok in info[“date")["result”]:
self. zookeepers append (Zookeeper (hostname=zook["metric”]["instance"].split(":")(@], region=zook[“metric”][“site”]}}
zookeeper_list += #"{self .zookeeper's[ -1].hostnane}:{self.zookeepers[-1] .listen_port},”
zookeeper_list.removesuffix(”,”)
return zookeeper_list
def connect_to_random_broker(self) -> Broker:
for broker in self.brokers:
try:
broker._connect(KAFKA_AUTH[“username”], KAFKA_AUTH["password”]}
return broker
except:
pass
else:
raise HTTPException(status.HTTP_417_EXPECTATION FAILED, “Couldnt connect to any broker")
def partition _reassignment(self, topic: str, partitions: list):
broker = self.connect_to_random_broker()
if partitions == ("*"]:
topic _to_move ~ {"topics": [{"topic": topic}], “version”: 1}
else:
topic_to move = {"topics": [{"topic™: topic, “partitions”: [{"partition": int(p}} for p in partitions]}], "version": 1}
broker._execute(F"echo ‘{Json.dumps(topic_to_mave)}‘ > hurricane. json")
conmand = REASSIGN_COMMAND.replace("<zook>", self.get_zookeepers()).removeprefix(",”)
proposed = broker. execute(comnand + f° --broker-list ~{",".join(str(i) for i in range(1, len(self.brokers) + 1))}"' +" --topics-to-move-json-file hurricane. json --generate”).split(“Proposed partition
reassignment configuration"}[4]
broker. execute(fecho ‘{proposed}* > hurricane_generated.json")
broker. execute(#’ export KAFKA_OPTS="-Djava. security auth. login.config-/hone/kafka/config/client_jaas.conf” && {command} --reassignment-json-file hurricane_generated.json --execute’)
return broker._execute(f' {command} ~-reassignment-json-file hurricane_generated.json --verify")
I@OSNiFi.py I
from pydantic import constr, BaseHodel
from typing import List
from .OCP import Pod, Cluster
from ..config import REQUESTS_CLIENT, MIFI_AUTH, WIFI_DB, NAAS_DB, CLUSTER_MANAGER, OPENSHIFT_API_SCHEME, TIMEZONE
from ..utils.general import timeout:
from fastapi.exceptions import HTTPException
from datetime import datetime
import re, time
class Node(Pod):
url: constr(max_length=256) = None
connected: bool
node_id: constr(max_length=128) = None
cluster_uri: constr(max_length=$12) = None
¢luster_token: constr(max_length=2043) = None
def get_connection(self):
eee ere ne re eee eE cLecenn nets "dad€d nnd lenntantinntrluctan bandane tt Caruana Authanientina Bannan": €"fealé cluctan tabant") uandfuctalen\ deanf\f*etuctan" 1 nadac®1

ee eee een ee me
def get_connection(selt):
req = REQUESTS. CLIENT.get(self.cluster_url + "/nifi-api/controller/cluster”, headers+{"__Secure-Authorization-Bearen": f"{self.cluster_token}"}, verify=False).json()["cluster”]{"nodes”]
status = next(ist(filter(lambda x: self.name in x{"address"], req)))["status”]
self.connacted = True if status == “CONNECTED” else False
return status
def connect_node(self):
req = REQUESTS CLIENT. put(self.cluster url + #"/nifi-api/controller/cluster/nodes/{self.node_id}", headerse{"__Secure-Authorization-Bearer": f"{self.cluster_token}"}, json-{"nade": {"nodeId": self.node_id,
“status”: “CONNECTING’}}, verify-False)
return req. status_code == 208
@itimeout(6@, “Node Is DISCONNECTED”)
def wait_for_node_to_connect(self, url, token):
while not self.connected:
connected = seif.get_connection(url, token)
4f connected == "DISCONNECTED":
raise "Node Ts DISCONNECTED”
‘vime.sleep(8.5)
return True
def delete_work(self):
self.exec_pod(["rm™, “-rf", “/var/Lib/nifi/work"))
def delete_state(self):
self.exec_pod(["rm", “-rf", */var/lib/nifi/state"])
I
def deiete_database(self):
sel¥.exec_pod([“en", “-rf", “/var/Lib/nifi/database_repository"))
def backup_and_delete_flow(self):
self.exec_pod(["mv", “/var/Lib/nifi/conf/Flow.xml.gz", "/var/lib/nifi/conf/flow.xml.gz.b¢"])
self.exec_pod([“nv", “/var/lib/nifi/conf/flow. json.gz", "/var/1ib/nifi/conf/flow.json.gz.be"])
def clean_old_logs(self, pattern):
self.exec_pod([“/bin/bash”, "-c", f'cd /var/lib/nifi/logs; neweste$(1s -t | grep “{pattern}” | head -n1); 1s | grep “{pattern}” | grep -vE “*$newest$” | xargs rm -F'])
def clean_journals(self):
self.exec_god(["/bin/bash", ’-c", ‘cd /var/Lib/nifi/Flowfile_repository/journals; newest=$(1s -t | head -nl); find . -maxdepth 1 -type | -name “$newest” -exec rm -f {} \3°])
def clean_ali_old_logs(sel¥):
self.clean_old_logs(“nifi-app*")
self.clean_old_logs("nifi-user*")
ee eke hanntebnnneey

def clean_all_old_logs(self):
self.clean_old_logs(“nifi-spp*")
self.clean_old_logs("nifi-user*”)
self.clean_old_logs("nifi-bootstrap*")
def cleanup_and_reset_node(self):
self.delete_pod()
sel¥.delete_database()
self delete_state()
sel¥.delete_work()
self.backup_and_delete_flow()
self.delete_pod_force()
class NiFi(BaseHodel):
nodes: List{Node] = []
network: constr(max_length=256) = None
name: constr({max_length=256) = Wone
url: constr(max_length=256) = None
openshift_url: constr(max_length=256) = None
region; constr(max_length=256) = None
openshift_cluster: constr(max_length=256) - None
token: constr(max_length=2048) = None
cluster: Cluster = None
class Config:
arbitrary types_allowed = True
def _init (self, **data):
super().__init__(*#data)
self.connect_and_get_details() I
@classmethod
def fron_node(cls, node: str):
if node[-1}.isdigit():
return cls(name-re_search("(.*)-[@-9]+", node).group(1))
return cls(name=node)
def determain_region(self):
self.region = self openshift_url.split(“ocp4~")[1]-split(”.")[@]
self.openshift_cluster = f"ocp4-{self.region}”
def get_nifi_token(self):
try:
REQUESTS_CLIENT. cookies .clear(donain=f" .{self.url-split("://")[1].split("/"}[@]}'}
access token = REQUESTS_CLIENT.post(self.url + '/nifi-api/access/token', data-NIFI_AUTH, headers-{“Content-Type": “application/x-wat-form-urlencoded"}, verify=False)
if access_token.status_code > 399:
raise Exception()

aa a ee a
access_token = REQUESTS_CLIENT.post(self.url + ‘/nifi-api/access/token’, datasNIFI_AUTH, headers={“Content-Type": “application/x-wuw-form-urlencoded jy versiy"v arse)
if access_token.status_code > 399:
raise Exception()
except:
raise HTTPException(40a, detail-f"Error: cant get token might be non-existant fron {self.nane}")
self.token = access_token. text
def check token(self):
if "_Secure-Authorization-Bearer” in REQUESTS_CLIENT.cockies.get_dict(domain=f” .{self.url.split(”://")[1].split("/")[0]}')-keys():
Self.token = REQUESTS CLIENT .cookies.get(“__Secure-Authorization-Bearer”, donain-f*.{self.url.split("://")[1].split("/"}[@]}")
if not self. token:
return False
try:
req = REQUESTS_CLIENT.get(self.url + ‘/nifi-api/access/token/expiration’, headers={"_ Secure-Authorization-Bearer”: f"{self.token}"}, verity-False)
if req.status_code > 299:
return False
return True
except:
return False
def connect_ocp_cluster(self):
self.cluster = CLUSTER MANAGER. get_cluster_conneciton(self-openshift_cluster, NIFI_AUTH[“usernane”])
if not self.cluster:
self-cluster = CLUSTER_MANAGER.connect_cluster(OPENSHIFT_API_SCHEME.replace("<openshift_cluster>", self.openshift_cluster), self.openshift_cluster, NIFI_AUTH["usernane”], NIFT_AUTH["password”],
region=self.region)
def connect_and_get_details(self): # Blame Sierra for this pile of shit
maas = True
if self name: I
info = NAAS_0B.nas_deployaent.find_one({"name”: self.name}, {"_id": })
if not info:
info = NIFI_DB.nifienvironnents.find_one({“name": self.nane}, {"_id": €})
naas = False
elif self.url:
self.url = self.url.split(”://7)[1]-split("/7)[0]
info = NAAS_DB.nas_deployment.find_one({"URLs.nifiurl”: {"$regex”: self.url}}, {"_4d": €})
if not info:
info = NIFI_OB.nifienvironments.find_one({“url": {"$regex”: self.url}}, {“_id": @})
naas = False
else:
raise HTTPException(4e0, detail-f*Error: cant connect without name or url")
if not info:
raise HTTPException(4a@, detailef"Error: cant find {se]f.name}”)
if naas:
self .url = info["URLs"][”nifiurl”].split(“/nift-api”)[e]
self.openshift_uri = info(“URLs"][“statefulsetUr]”]
self.name = info[“name"]

self.url = Anfo["URLs"]{"nifiurl”]. split("/nifl-api”)(@]
self.openshift_url = info[“URLs”}["statefulseturl"]
self.nawe = info[“nane"]
else:
self.url = info{"nifiurl"].split("/nifi-api”)[@]
self_openshift_uri = info["openshifturl"]
self.name = info[“name"]
self.determain_region()
sel¥.connect_ocp_cluster()
if self.check_token(}:
return True
else:
try:
self.get_nifi_token()
except:
pass
return True
def get_nodes(self):
state = ""
now = datetime. now(TIMEZONE)
pods = self.cluster.get_namespace_pods(self.name)
try: nades = REQUESTS CLIENT.get(self.url + “/nifi-api/controller/cluster”, headers={"__Secure-Authorization-Bearer”: #"{self.token}"}, verify-False). json()["cluster"][*nodes”]
except: nodes = {J
for pod in pods. items:
if not pod.metadata.nane.startswith("zk-"):
if next(iter(pod.status.containerStatuses[@].state))[@] -- "waiting":
if pod.status .containerStatuses[@].state.waiting.reason == “CrashLoopBackOff”: I
state = "CrashLoopBackoff”
if pod.status.containerStatuses[0].state.waiting.reason =< “ImagePullBackOff":
state = “ImagePuliBackort”
else:
tinestamp_with_offset = pod. status.containerStatuses[(0].state.running.startedAt .replace(’Z", “+00:00°)
delta < now - datetime.strptine(timestamp_with_offset, "KV-%m-ZdTRH:2M:%S%z") .astimezone( TIMEZONE)
Af delta.total_seconds() < 38 and pod.status .containerStatuses[@].restartCount > 4:
state = “CrashLoopBackOrf”
if nodes:
node = next((n for n in nodes if pod.metadata.name in n[“address”]})
else:
node = {}
self. nodes. append (Node(name=pod metadata. name,
nanespacecself .name,
cluster-self. cluster,
statesstate,
node_idenode.get("nodeId”, “"),
connected=True if node.get("status”, "COMNECTED”) == "CONNECTED" else False,
cluster_token=self.token if self.token else "",
cluster_url=self.url))
ee ae

connected-True if node.get("status", “CONNECTED") == "CONNECTED" else False,
cluster_token=self.token if self.taken else "",
cluster_urleself-url))
return self.nodes
def get_storage_stats(self):
storage = {}
if not self.token:
self.get_nifi_token()
head = {"Authorization": “bearer "+ self.token}
try:
nodes_stats = REQUESTS _CLIENT.get(self.url + "/nifl-api/system-diagnostics?nodewise-true”, headers-head, verify-False)
if nodes_stats.status_code > 399:
raise Exception()
except:
Paise HTTPException(5@8, f"Error getting stats on {self.name}")
for node in nodes_stats.json()["systesDiagnostics”]["nodeSnapshots”]:
storage[node[“address”].split(".")[@]] = {"content”: node{“snapshot™){"contentRepositoryStorageUsage”][@], “Flowfile”: node[“snapshot™]["FlowrileRepositoryStorageUsage” I}
return storage
class NAAS(NIFA):
owner: constr(max_length=256) = None
america_url: constr(max_length=256) = Hone
l@esocp.py
from pydantic import BaseModel, constr
fron fastapi import WebSocket
from typing import List
fron openshift.dynamic import DynamicClient
from openshift.helper.userpassauth import OCPLoginConfiguration I
ron kubernetes import client
from kubernetes.client.exceptions import Apitxception
from kubernetes.stream import strean
from ..utils.general import timeout
import time, asyncio
class Cluster(DynamicClient):
name: conste(nax_length=256) = None
region: constr(max_length=256) = None
token_expires: int = @
def check_token(sel#):
if time.time() > self.token_expires:
return False
return True
def get_namespace_pods(self, namespace: str}:
return self.resources.get(api_version="v1‘, kind="Pod*).get(namespacesnamespace)

def get_namespace_pods(self, nawespace: str):
return self.resources.get(api_version="vi", kind="Pod’).get(namespace=namespace)
class Clustertanager(BaseModel):
clusters: List[Cluster] = {]
class Config:
arbitrary types_allowed = True
def get_cluster_conneciton(self, cluster_name: str, username: str = None) -> Cluster:
for cluster in self.clusters:
if cluster.name == cluster_name:
if username:
if cluster.configuration.ocp_username != username:
continue
if cluster. check_token():
return cluster
else:
self.clusters.renove(cluster)
else:
return None
def connect_cluster(self, api_url: str, cluster_nase: str = None, username: str = None, password: str - None, tokensNone, region="") -> Cluster:
if not username and not password:
if token:
config = OCPLoginConfiguration(api_uri, api_key-{"authorization”; “Bearer “ + token})
else:
raise Exception(“Username and password or token is required”)
else: I
config = OCPLoginConfiguration(api_url, ocp_username-username, ocp_password=password)
config.verify_ssl = False
if not token:
config.get_token()
api_cli = client .apiClient (configuration=config)
cluster = Cluster(api_cli)
cluster.token_expires = cluster.configuration.token["expires_in”] + tine.time()
cluster.name = cluster_name
cluster.region = region
self. clusters. append(cluster)
return cluster
class Pod(BaseModel):
name: conste(max_length-256) = None
cluster: Cluster = None
namespace: constr(max_length=256) = None
state: constr(max_length-256) = None

namespace: constr(max_length=256) = None
state: constr(max_length=-256) = None
class Config:
arbitrary types allowed = True
@timeout(6a, “Pod timed out deletion")
def wait_for_pod_deletion(self):
pod = self-cluster.resources. get(api_version="v1', kind="Pod" }.get(namespace=self.nanespace, name=self.nawe)
while pod.status.phase != “Pending”:
try:
pod « self.cluster.resources.get(api_version=‘vi', kind='Pod').get(nanespace=self.namespace, nane=self.name)
except ApiException:
pass
time.sleep(@.s)
return True
Gtimeout(63, "Pod timed out initializing")
def wait_for_pod_running(self):
pod = self.cluster.resources.get(api_version-"vi', kind="Pod").get(namespace-self. namespace, nane-self.name)
while pod.status.phase != “Running”:
try:
pod = sel¥.cluster.resources.get(api_version=‘vi", kind-"Pod").get(nanespace=self .namespace, name-self.name)
except ApiException:
pass
time.sleep(@.5)
return True
def exec_pod(self, command: list): I
api = client. CoreViApi (self.cluster. client)
return strean(api.connect_get_namespaced_pod_exec, self.name, self.namespace, conmand-conmand, stdin-False, stdout-True, stderr-True, tty-False)
def delete_pod(self):
pod = self. cluster.resources.get(api_version=‘vi', kind='Pod")
pod.delete(namespacecself.nanespace, name=sel¥.nane)
self.wait_for_pad_deletion()
self.wait_for_pod_running()
return True
def delete_pod_force(self):
pod = self.cluster.resources.get(api_version="vi', kind="Pod")
pod.delete(namespace=self.namespace, same-self.name, grace_period_seconds=0)
return True
def delete_pod_no_wait(self}:
pod = self.cluster.resources.get(api_version="vi', kind=‘Pod*}
ee be anak

def delete_pod_no_wait(self):
pod = self.cluster.resources.get(api_version="vi", kind="Pod")
pod.delete(namespace-self.namespace, name-seif.nane)
return True
async def stream_logs(self, websocket: WebSocket):
api = client. Corevivpi(self.cluster.client)
log_stream = stream(api.connect_get_namespaced_pod_exec, self.nane, self-nanespace, comnmand=["/bin/bash”,"-c","STAIL_LOGS™],
stdout=True, stderr=True, stdin-False, tty-False, _preload_content-False)
try:
while log_stream.is_open()+
Jog_stream.update(timeout=1)
if log_stream.peek_stdout():
await websocket.send_text(log_strean.read_stdout())
if log_stream.peek_stderr():
await websocket.send_text(log_stream.read_stderr())
await asyncio.sleep(@.1)
except Exception as e+
return
finally:
await websocket.close()
'@ngServiceNOw.py
# Code From Athena, Modified
from pydantic import BaseModel
from requests import get, post
from ..config import SERVICE_NOW_GROUP_ID, SERVICE_NOW USERNAME, SERVICE_NOW_PASSWORD, SERVICE_NOW_URL, SERVICE_NOW_ASSUME_USERNAME
class Incident (BaseModel): I
title: str
description: str
technology: str
dest_group: str
tequila: bool = False
class Servicenou():
def __ init__(self, username, password, api_url):
self.usernane = username
self.password = password
self.api_url = api_url
self.headers = {"Content-Type": “application/json”, "Accept": "application/Json"}
def get_call(self, call_number):
req = get(self.api_url, headers=self.headers, paramse{ ‘number’: cali_nunber},
auth=(self.username, self. password), verify-False)
return req.json()

ee 7 _
auth(self.username, self.password), verify-False)
return req.jsen()
class Servicetiow(ServiceNow):
def init__(self, usernane, password, api_url):
super()-__init__(username, password, api_url)
# Adds attributes to the incident body (what we will see in the incident
self.incident_body = {
“category”: “ae",
“subcategory”: "ams",
“state; “oTm",
“contact_type": “Self-service”,
“location”: “7188.n7g8n.n12~22",
“u_department™: "74887277",
"impact": 2,
"urgency": 2,
“u_perational_ispact”: “Hurricane”,
“network”: “orn 027
+
def get_user_id(self, username):
req = get(F”{SERVICE_NOW_URL}/now/table/sys_user?user_name=(usernane}&sysparm fields=sys_id&sysparm_limit-3",
auth=(self.username, self.password), verifysFalse)
return req. json()[ “result” ][O]["sys_id™}
def get_user_info(self, user_id):
response = get(FW{SERVICE_NOW_URL}/now/table/sys_user/{user_id}”,
auth-(self.username, self.password), verify-False)
return response. json() I
def get_service_details(self, technology):
response = get(#"{SERVICE_NOW_URL}/now/table/cadb_rel_ci?syspara_queryachild.naneL IKE(technology}%Separent .nameLAke&sysparm_fields-parent, child, parent.sys_id,child.sys_id&sysparm_display_value=true",
authe(self-username, self.password), verify-False)
return response. json()
def get_group_details(self, search):
response = get(f"{SERVICE_NOW_URL}/now/table/sys_user_group?sysparm_query=nameLIKE{ search}&sysparm_Fields=sys_id&sysparm_limit=3",
autha(self.username, self-password), verify-False)
if response. json():
return response. json()["result”][@}{"sys_id”]
response = get("{SERVICE_NOW_URL}/now/table/sys_user_group?sysparm_query=emailLIKe{search}&sysparm_fields-sys_id@sysparm_limit=3",
auth=(self.username, self.password), verify-False)
if response. json():
return response. json()[“result”][@]["sys_id”]
return SERVICE_NOW_GROUP_ID

EE NE
return response. json(){"result”](@]{"sys_id"]
return SERVICE_NOW_GROUP_ID
def post_call(self, title, description, technology, dest_group, tequila):
user_id = self.get_user_id(SERVICE_NOb_ASSUME_USERNAME)# Gets the user id of the user who opened the incident
user_info = self.get_user_info(user_id)# Gets the user info of the user who opened the incident
service_info = self.get_service detai1s(technology)
# Adds the description to the incident body
self.incident_body[“description"] = description
self.incident_body[“u_phone voip] = user_info[ ‘result” ][’u_phone_voip*]
self incident_body["u_wobile_phone"] = user_infof ‘result’ }{‘u_phone_voip’ ]
self-incident_body["u_computer_name”] = user_info["result']['u_phone_voip"]
self.incident_bady["opened_by”] = user_id
self.incident_body[“caller_id™] = user_id
self. incident _body{"service_offering”] = service_info[ "result" ][@]{"child"]["display_value"]
self.incident_body["business_service"] = service_infof “result” ][@][ "parent" ][“display_value”]
self.incident_body[“short_description"] = title
self. incident_body["u_impact_technology"] = "S9@f3178148a8e5e20d266d3b1ed658c"
self.incident_body[“assignment_group"] = self.get_group_details(dest_graup)
if tequila:
self.incident_body[“u_system failure”] = True
else:
self-incident_body[“u_system_failure"] = False
self incident_body{"u_open_for"] = “17w* 173"
# Posts the API request to open a new incident
response = post(self.api_url, headers-self-headers, json-self. incident_body,
auth=(self.username, sel¥.password), verifysFalse)
# Returns the incident number after it has been opened
return response. json()
I
def create_incident(incident : Incident):
snow = ServiceNow(SERVICE_NOW_USERNAME, SERVICE_NOW_PASSWORD, #7 {SERVICE_NOW_URL}now/table/incident?sysparm_display_value=true’)
return snow.post_call(titlesincident.title,description-incident.description, technology=incident.technology, dest_group=incident.dest_group, tequila-incident tequila)
def get_incident (incident_number):
show = SePviceNow(SERVICE_NOH_USERNAME, SERVICE NOH PASSWORD, f°{SERVICE_NOW_URL}now/table/incident?sysparm_display_value-true")
return snow. get_call (incident_number)
1@HShMQ. py
from .Host import Host
from ..config import VICTORIA_QUERY_METRICS_URL, REQUESTS_CLIENT, DEFAULT_MQ_ CHANNEL, MQ_AUTH
from fastapi import HTTPEXxception, status
rom pydantic import constr
import. pymqi,
class QMGR(Host):
qm: conste(max_length=256) = None
Listen_port: constr(max_length=256) = None
nee ey | nannen

class QHGR(Host):
gn: constr(max_length=256) = None
listen_port: constr(max_length=256) = None
version: constr(max_length-256) = None
pymgi_handier: pymqi.QueueManager = Hone
def __ init__(self, **kwargs):
Super().__init__(**kwargs)
self.get_connection_details()
class Config:
arbitrary_types_allowed = True
def get_connection_details(self):
Info = REQUESTS_CLIENT.get(VECTORIA QUERY METRICS_URL + f*wmq_qngr_info{{queue_manager="{self.qu}"}}').json{)
if not info[ ‘data’ ][ ‘result’ ]:
raise HTTPException(status-HTTP_4@4_NOT_FOUND, “Queue Manager Not Found")
self.Listen_port = info[ ‘data’ }[ result‘ ][@]{ ‘metric’ ][*port"]
self.hostnane = info[ ‘data’ ][ ‘result’ ][@J[‘metric’ ][‘host’]
if not self.ip:
self .dns_to_ip()
def get_qn_by_host(self):
info = REQUESTS_CLIENT.get(VICTORIA_QUERV_METRICS URL + #*wmq_qngr_info{{host="{self.hostname}"}}")-json()
self.qu = info[ ‘data’ ][‘result’ ][@]["netric’ }[ ‘queue_manager’]
def connect(sel):
if self.pymgi_handler: I
if self .pymgi_handler.is_connected:
return “Already Connected”
if not self.qm and sel¥.hostnane:
self.get_qn_by_host()
Af not self.ip or not self.hostname:
self .get_connection_details()
if not self.ip or not self.listen_port:
raise Exception("Not Enough Details For Qa”)
self.pynqi_handler = pymqi.connect(self.qm, DEFAULT_MQ CHANNEL, f*{self.ip}({self.1isten_port})"}
def disconnect(self):
sel¥.pymgi_handler.disconnect()
def reset_channel (self, channel_name):
cee nner ent IE eecnt handtaa

ee Ee ee
def reset_channel(sel¥, channel_name):
mq_pcf_exec = pynqi.PCFExecute(self .pymqi_handler)
channei_config = {
pymgi.CHOCFC.MOCACH_CHANNEL_NAME: channel_name,
y
nq_pcf_exec .MQCHD_RESET_CHANNEL (channel_config)
return True
def restore_from_diq(self):
sel¥._connect_and_execute_no_output(MQ_AUTH["username”], MQ_AUTH["password"},
f*pkill ~9 .*runmgdlg.*; echo “ACTION (RETRY) | timeout -s 9 6s /opt/nqn/bin/runngdlq SYSTEM.DEAD.LETTER.QUEUE {sel¥.qn}")
retupn True
1@#$Zookeeper.py
from .Host import Host
from pydantic import constr
class Zookeeper(Host):
Listen_port: constr(max_length=5) = "21817
def echo():
return True
1@#$__init__-py
1@9$_init__.py I
1@NSauth. py
feom fastapi.responses import RedirectResponse
from fastapi.routing import APIRouter
fron fastapi.security import APIkeyCookie
from fastapi import Depends, Response, Request
from ..config import ASYNC_REQUESTS_CLIENT, GRIFFIN AUTH_URL, AVATAR_URL
from datetime import timedelta
from tine import time
COOKIE_SCHEME = APIKeyCookie(name="access_token”}
async def get_logged_user(access_token: str = Depends(COOKIE_SCHEME)):
request = avait ASVYNC_REQUESTS_CLIENT.get(f*{GRIFFIN_AUTH_URL}/authorization/validate?tokene{access_token}”, verify ssl-False)
if request.status > 299:
return False
request = await ASVNC_REQUESTS_CLIENT.get(#*{GRIFFIN_AUTH_URL}/authorization/getClains?token={access_token}", verify ssl-False)
response = await request. json()
cee wy ENE ewan gad fen cnenenf 'eAthAerrunthinme! Te

ee EES
return False
request = await ASVNC_REQUESTS CLIENT. get(f*{GRIFFIN_AUTH_URL}/authorizstion/getClains ?token=(access_token}", verify_ss]-False)
response = await request. json()
response[“avatar"] = f"{AVATAR_URL}/{response[ "sAMAccountName* ]}”
return response
router = APIRouter(tags=["Auth"], prefix-"/auth")
@router.get(“/login")
async def login():
return RedirectResponse("/authentication?tokenConsumerURL=/auth/me” )
@router.get("/me", include_in_schena=Faise)
async def get_user(request: Request, response: Response, token = ""):
user = await get_logged_user(token)
if user:
response. set_cookie(key="access_token”, valuestoken, expires-tinedelta(user[“exp"] - 48 - int(time()}}, httponly-True)
response.status_code = 387
response.headers{"Location™] = “/"
retura True
if "access_token” not in request. cookies.keys():
return RedirectResponse(’/login’)
return await get_logged_user(request.cookies[“access_token”])
@router.get(”/auth_example”)
async def example_auth_required_endpoint(current_user = Oepends(get_logged_user)):
return current_user
Y@rsgeneral .py T
import signal
def timeout(seconds=38, error_message=‘Function call tised out’):
def decorator(func):
def _handle_timeout(signum, frame):
raise TimeoutError(error_message)
def wrapper(*args, **kwargs):
signal.signal(signal.SIGALRM, _handle_timeout)
signal. alarm(seconds)
try:
result = func(*args, **kwargs)
finally:
signal. alarm(@)
return result
return wrapper
return decorator

return wrapper
return decorator
l@e$logging.py
from elasticsearch import Elasticsearch
inport logging
from time import tise
class ElasticsearchHandler (logging Handler):
def init__(self, urls, index, username, password):
super(}._init_()
self.urls = urls
self. index = index
self.usernane = username
self.password = password
self.connection = Elasticsearch(hostseself.urls, http_auth=(self.usernane, self.password), verify certssFalse, ssI_show_warn=False)
def emit(self, record):
try:
msg = self. format(record)
#self. connection. index(index=f" {self index}-{str(datetime.now(pytz-timezone("Asia/Tel_Aviv™)).strftime("2¥-%a-2d"))}', body=msg)
self connection. index(index=self index, body=msg)
except Exception as e:
print(F"Couldn’t Index Log: {e}")
def format(self, record):
return {
‘message’: record.getMessage(),
"level": record.levelnane,
*@timestanp': int(time()),
*autonation_Ad': getattr(record, ‘id’, Wone), I
“name': getattr(record, ‘automation_name’, None),
‘progress’: getattr(record, ‘progress’, None),
‘parameters’: getattr(record, ‘parameters‘, None),
‘state’: getattr(record, ‘state’, None)
}
l@e$task_manager.py
import asyncio, inspect, logging
from fastapi-exceptions import HITPException
from fastapi.responses import RedirectResponse
from fastapi import webSocket
from celery import Celery
from celery.result import AsyncResult
rom functools import wraps
from celery.utils.log import get_task_logger
fron .logging import ElasticsearchMandler
from copy import copy
from celery.schedules import crontab
from typing import Any
from ..src.Archive import ArchiveAlert
neem pra CI ACTTFC Cabral IGIE ELACTTFEEAGRU Tency rclenw Taye THmtr VAEWA TACVE EAATCTDAN CCDIKUG PEICRY TAGYS fnlbceTind Fcecow Tasen rramceTeng RETATIE VACKA AITTTMATTNOT AITTU

ee
from celery.schedules import crontab
from typing import Any
from ..src.Archive import ArchiveAlert
from . config import HURRICANE_AUTH, ELASTICSEARCH_URLS, ELASTICSEARCH INDEX, CELERY_TASKS TOPIC, KAFKA_TASKS_BOOTSTRAP_SERVERS, CELERY TASKS COLLECTION, CELERY_TASKS_CONNECTION_DETATLS, KAFKA_AUTOMATIONS_AUTH,
DOMAIN_FQDN, AUTOMATIONS_ELASTIC_TRACK
class AutomationManager(Celery):
task_registry: dict = {}
async def list_tasks (self):
tasks = copy(self. tasks)
for task_name, task_obj in self.tasks.items():
if “celery.” in task name:
‘tasks. pop(task_nane, None)
return tasks
async def list_autonations(self):
auto = {}
for task_name, task_obj in self.tasks.items():
if “celery.” in task_name:
continue
autoftask_name] = {“description™: f~{self.task_registry[task_name]["description’]}",
“arguments”: {k: str(v) for k, v in self.task_registry[task_nane][“argunents”].items()}}
return auto
async def list_tasks_names(self):
return [task_name for task name in self.tasks.keys() if “celery.” not in task_name]
I
async def get_task(self, task_name):
return self.tasks[task_nane]
async def get_args(self, task_name):
args = self.task_registry[task_name]{“arguments”].itens()
return {k: str(v) for k, v in args}
async def run_task(self, task_name, **kwargs):
await self.check_args(task_name, kwargs)
res = await asyncio.to_thread(self.send_task, task_name, args-[False], kwargs=kwargs)
return await asyncio.to_thread(res-get)
async def schedule task(self, task_name, **kwargs):
await self.check_args(task_nane, kwargs)
return await asyncio.to_thread(self.send_task, task name, args=[False], kwargs=kwargs)
ce reek nptpernt inn nama anchdon wibuamnets

eee oa
from celeepusshadaiesaimpoke.toonhabad(self.send_task, task_nane, args=[False], kwargs-kwargs)
from typing import Any
from ..src.archive import ArchiveAlert
Fronasynondé§ import lHvaRECRHECAETHsuEDASTIOSEAREE,URESDIERASTERHEAREY:TNDEX, CELERY_TASKS_TOPIC, KAFKA_TASKS BOOTSTRAP_SERVERS, CELERY_TASKS COLLECTION, CELERY_TASKS_CONNECTION_DETAILS, KAFKA_AUTOMATIONS_AUTH,
DOMATN_FQDH; tAYEORATRONE tgs TAOL@MACKON_name, kwargs)
guto = await asyncio.to thread(self.send_task, automation_name, args«[dict(archive) if len(dict(archive); != @ and archive else False], kwargsekwargs)
return RedirectResponse(AUTOMATIONS_ELASTIC_TRACK.replace(“<automation_id>”, auto.id))
class AutomationHanager (Celery):
bagkcregisthyckdacgsés€}¥, task_name, args):
# Check if the task exists
asyné fdeSsiistmeashs (de1691f task registry:
‘tasksaéseopy(Betfepaitsy status_code=404, detail-"Task not found”)
fothtabkifiakes tagknebisierself.thekeighems{pe and no garbage args
For afg"coheryaPginatankinapegs.items():
expetackstpop(tasklfameskNong}stry[task_nane][”arguments”].get(arg_name, Exception)
retueh tapksted_type is Exception:
raise HTTPException(status_code=4@0, detail-f"Got unexpected argument {arg_name}")
if expected_type is not None and not isinstance(arg_value, expected_type):
async def listisatunationa(sélf}status_code=400, detail-f"Invelid type for argument {arg_name}"}
auto = {}
for task name, task_obj in self.tasks.items():
async aekflfeelerycR_tastasklfgmesbsocket: WebSocket, task_id):
res = Asgontdnuet(task_id)
«nilautoftaskinane]rei(“desthiptignts fl{se]f.task_registry[task_name]("description’ ]}",
avait asyncio.slesptarguments": {k: str(v) for k, v in self.task_registry[task_nane](“argunents”].items()}}
retuenzauto2bsocket.sena_jso-/{"id". task_id, “state”: res.state, “progress” res.info})
retucr await as."c¢io.to_threadires get,
async def list_tasks_names(self):
2: "petatneptaskznamesfor task,namecindself.tasks.keys() if “celery.” not in task_nane]
rez = eseechesult tesk_id
St aua.n goed: to cheese res reaay I
async def-get-task(selfyvtask:mame)222 "2:.g27
return selfttasks[task.namaj:- -25.2-+-
async-def-get.args(self,stask_name):2122- 9 {72 .dse0 ureieare come @toretac73
-:. =: angs = self.task:registry[task.nane][~argunents”].items()
2. neturn {k-str(v)-foecks win args == 5077
7 te =a : ciple feo Aer pooplite ue ah a Tate LILA elit
async def run_task(self, task_name, **kwargs):- ~ = Poses - e i a
avait sel¥.check_ergs(task_name, kwargs) - ~=
res = await asyncio.to_thread(self.send.task, task_name, args=[False], kwargs-kwargs)
return await asyncio.to_thread(res.get) - -
async def schedule task(self, task_name, **kwargs):
await self.check_args(task name, kwargs)
return await asyncio.to_thread(self.send_task, task name, args-[False], kwargs-kwargs)
banners

ee
task_mansgeornoatatisespnaity. soathedad(Sed€.send task, task name, args-[False], kwargs=kwargs)
task_manager.conf .broker_connection_retry_on_startup = True
task manager-conf-result backend_options = {"database’ : CELERY_TASKS_CONNECTION_DETATLS["database"], ‘taskmeta_collection’: CELERY_TASKS COLLECTION}
taskasynagdefcachedudgodiebackedd, setémgtien(name, archive, **kwargs):
“optamait se]f-check_args(automation_name, kwargs)
dabonsoamast ; aéfnERs, TaSKbreadselfoucnsTtakk[ “datebatkOh_name, args-[dict(archive) if len(dict(archive)} != @ and archive else False], kwargs~kwargs)
} return RediractResponse(AUTOMATIONS ELASTEC_TRACK.replace("<autonation_id>”, auto.id))
t
task_manager. conf. result_backend=CELERY_TASKS_CONNECTION_DETAILS[“string”]
taskanynagdefcohéckeatgs(bedajetask’nabefiarge)e_alerts Smin‘: {"task': ‘autofix_scan_alerts’, “schedule”: crontab(minute='*/5’)}}
# Check if the task exists
if task_nane not in self.task_cegistry:
def automat inal setdTTeExpaption¢stotysicodtaddapadétal]-AisekinotAfiguad?31se) +:
def #eCheakoitfthe)argunents are of the right type and no garbage args
fonsbrganames angkYblud-Invergsaitesiag}:__name__, tags-[func.__module_])
furapspécted_type = self.task_registry[task_name]["arguments”].get(arg_name, Exception)
def dfapnpecteiftyperes, EXCeptigs) :
loggeaisegTTPExteptiga(statdsnend’=400, detail-f"Got unexpected argument {arg_name}”}
iSgexpectetetypéliggaog.Wuke)and not isinstance(arg_value, expected type):
LoggeaiseoMFEBEnception¢status_code=400, detail-f* Invalid type for argunent {arg_name}")
if aot any(isinstance(handler, ElasticsearchHandler) for handler in logger handlers):
async def Livegthackleask(S&ifsimebsockutodWebSOckEESCEBSRCHADELS, ELASTICSEARCH INDEX,
res = AsyncResult(task_id) HURRICANE_AUTH["username”], HURRICANE_AUTH[“password”])
while notoggaitaasynodoctoi thread(ées}ready):
await asyncio,sleep(0.1)
amakterwebSocketssends4sdn(4{Med”: akaSkaidsd"statera-Gescstate] {peageess!2,res.info})
return await asyncio.to_thread(res.get) automation_name’) se_f.1am2,
“progress "OR,
"state STARTED’ ,
async def get_task_status(self, task_id): oeramecers': kuargs”)
res = AsyncResult(task_id)
1 await sasyncio:terthread(reszready):=-"" I
eetura dwaitzasyncioxto;thread(pesiget) r2cs-0-tou"
return alait-asyncio.to.thread({res,info) -<i- rec_2it iv
gov vecicr_aswe ge frame,
cecgrszi = f.nav cucract. flcec tects: steps *10R EF kif Luuncceecaitme cotasteps y favat, int) tase te
task_manager = AutomationManager (‘task manager; =inclades{ "hurticane-home.automations”])
‘task_manager.conf-task-serializer-=.' json’: 20222
task_manager.conf.task_default_queue = CELERY_TASKS_TOPIC
‘task_manager.coné.broker_url.= “confiuentkafka://* + 7;7.join([f"{b. split(':*)[@)}-{DOMAIN_FQON}:{b. split(*:')[1]}" for b in KAFKA_TASKS_BOOTSTRAP_SERVERS.split(”,"}])
task_manager.conf.broker_transport_options = {"kafka_common config":{ ©: 1? =7+
oo “bootstrap. servers’ ::"," join([f"{b-split(‘:'}[8}}.{DOMAIN_FOON}:{b. split(":"}(2]}" for b in KAFKA_TASKS_BOOTSTRAP_SERVERS.split(",")]),
“sasl.mechanisa’; 'SCRAM-SHA-256',
“security.protocol': "SASL_PLAINTEXT",
‘sasl.username': KAFKA_AUTOMATIONS_AUTH[ ‘username’ ],
*sasl.password': KAFKA_AUTOMATIONS_AUTH[ ‘password '],,
“client.id': KAFKA_AUTOMATIONS_AUTH[ ‘username’ ],,
*group.id" : KAFKA_AUTOMATIONS_AUTH[ ‘username’ ]
i
task_manager.conf.task_track_started = True
‘task_manager .conf.broker_connection_retry_on_startup = True
task manager. conf.result_backend_options = {“datebase’: CELERY_TASKS CONNECTION _DETATLS["database"], ‘taskmeta_ collection’: CELERY_TASKS COLLECTION}
ee use og

i aa a
task_manager.conf.task_track_started = True ‘automation_nane’: self.name,
task_manager.conf .broker_connection_retry_on_startup = True ‘progress’: °N/A’,
task_managerconf.result_backend_options = {database": CELERY_TASKS_CONNECTIONSOGTAILS[FAaLEBdse"], ‘taskmeta_collection': CELERY_TASKS_COLLECTION}
task_manager.conf.mongodb_backend_settings = { ‘parameters’: kwargs})
“options” : faise e
“authSource”: CELERY_TASKS_CONNECTZON_DETAILS["database™]
} logger info(#'Task {self.name} has Finished: {str(res)}', extra={"id'; self-request.id,
} *automation_name': self.nane,
‘task_nanager conf .result_backend=CELERY_TASKS_CONNECTION_DETATLS["ptogng34": '100.00%",
task_manager.conf.beat_schedule © {‘autofix_scan_alerts Smin": {"taskte’ !adsv€igsstan alerts", ‘schedule’: crontab(minute-"*/5')}}
“parameters’; kwargs})
try:
def automation(tétakegtép}+1, description="Automation", AutoFix: Any = False):
def decorator(funtget = ArchiveAlert(**args[@])
@task_manageeltesk¢bindépedey naét{fang7 name, tags=[func._module_})
@uraps (func)asyncio. run(alert.send_to_archive())
def arappertsedft iargs,et*kwargs) +
logger iat gétcbadtinloggertiedthdame) {self request.id} :{str(e)}:{args}")
Loggen.setLevel (logging. INFO)
logger.propagate = False
Add the function to the registry
sig &finoheeny¢igiasteacéhahdler, ElasticsearchHandler) for handler in logger.handlers):
task_mandggrhandberesi Bias ffaseanchHand]ér¢ELASTEOSEARGH : URDSpcE LASTZESEARGHAINDEX;f param.annotaticn is rot inspect.Parameter.empty else Nene
HURRTCANBAUTHE Sasérnang” JarHURRECANETAUTH Tpassmecd?}}ion”: description, “autofix”: AutoFix}
task_nandgggeteaddMagdier(1égrhandien}__]["arguments”] .pop("self’, None)
reture arapoer
return doggercinfo(f*Task {self.name} has Started’, extra={"id': self.request.id,
*automation_name": self.name,
‘progress’: "OX",
‘state’: "STARTED',
‘parameters’: kwargs})
def update_progress(current, output=""): I
self. update_state(state="PROGRESS‘, meta-output)
dogger.info(output, extrae{*id’: self.request.id,
‘automation_nane': self.name,
‘progress’: #°{Float(current)/float(total_steps)*1e0: .2f}%" 1 isinstance(type(total_steps), (float, int)) else "2%",
“state‘’: ‘PROGRESS‘})
self.update_progress = update_progress
sig « inspect.signature(func)
defaults = {nane: paran.default for name, param in sig.parameters.items()
if paran.default is not inspect.Paraneter.empty}
for nae, default_value in defaults. items():
Af name not in kwargs:
kwargs[nane] © default_value
try:
res = func(self, **kwargs)
except Exception as e:
logger.error(f‘Task {self.name} has Failed: {str(e)}’, extras{‘1d’: self.request.id,
‘automation nana": self.nane,
"progeess’: f°N/A",
‘state’: ‘FAILED’,
Peneeee ss kianeh

rh = Daa a
‘automation_name’: self.name,
‘progress’: #°N/A‘,
‘state’: "FATLED',
‘parameters’: kwargs})
raise e
Logger -info(f'Task {self.name} has Finished: {str(res)}", extra={"id': self.request.id,
‘automation_name’: self.nane,
“progress': '180.00X",
“state”: ‘SUCCESS’,
‘parameters: kwargs})
try:
if args{e):
alert = Archivealert(**args[@])
alert.description = f*{res}”
asyncio.run(alert.send_to_archive())
except Exception as e!
print(f*Couldn't archive Alert {self.request.1d}:{str(e)}:{args}”)
return res
# Add the function to the registry
sig = inspect.signature(func)
task_nanager.task_registry[func.__name_] = {"arguments”: {name: param.annotation if param.annotation is not inspect.Parameter.enpty else None
for name, param in sig.parameters.items()}, "description": description, “autofix": AutoFix}
task_manager.task_registry[func.__name__J[”arguments”].pop(‘self*, None)
return wrapper
return decorator
I
